{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from time import sleep\n",
    "import sys\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Activation, TimeDistributed, LSTM, Dropout\n",
    "from keras.preprocessing import image\n",
    "from keras import backend as K\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "import pylab\n",
    "from PIL import Image\n",
    "import imageio\n",
    "#imageio.plugins.ffmpeg.download()\n",
    "\n",
    "import scipy\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "EPOCHS = 20\n",
    "VIDEO_FOLDER = (os.getcwd() + '/videos/Clips')\n",
    "BATCH_SIZE = 32\n",
    "FRAME_SQUARE_DIM = 178\n",
    "FRAMES_PER_VIDEO = 30\n",
    "KEEP_PROB = 0.7\n",
    "TEST_SIZE = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Make Frames directories in Truthful and Deceptive folders\n",
    "if not os.path.exists((VIDEO_FOLDER + '/Truthful/Frames')):\n",
    "    os.mkdir(VIDEO_FOLDER + '/Truthful/Frames')\n",
    "if not os.path.exists((VIDEO_FOLDER + '/Deceptive/Frames')):\n",
    "    os.mkdir(VIDEO_FOLDER + '/Deceptive/Frames')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def resize_and_crop(frame, square_dim):\n",
    "    # get aspect ratio\n",
    "    aspect = frame.size[0]/frame.size[1]\n",
    "    \n",
    "    # resize to 178 on shortest side and keep original aspect ratio\n",
    "    frame = frame.resize((int(square_dim*aspect), square_dim))\n",
    "    \n",
    "    # crop 178 square from center\n",
    "    half_the_width = frame.size[0] / 2\n",
    "    half_the_height = frame.size[1] / 2\n",
    "    frame = frame.crop(\n",
    "        (\n",
    "            half_the_width - (square_dim/2),\n",
    "            half_the_height - (square_dim/2),\n",
    "            half_the_width + (square_dim/2),\n",
    "            half_the_height + (square_dim/2)\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    return frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_frames_from_videos(folder, frames_per_video, frame_square_dim):\n",
    "    \n",
    "    # get one video at a time in the folder\n",
    "    vid_counter = 0\n",
    "    for video_file in os.listdir(folder):\n",
    "        vid_counter += 1\n",
    "        \n",
    "        if video_file not in ['Frames']:\n",
    "            # printing status\n",
    "            sys.stdout.write('\\r')\n",
    "            sys.stdout.flush()\n",
    "            sys.stdout.write('Getting frames for video ' + str(vid_counter) + ' of ' + str(len(os.listdir(folder))))\n",
    "            sys.stdout.flush()\n",
    "            \n",
    "            video = (folder + '/' + video_file)\n",
    "            video = imageio.get_reader(video,  'ffmpeg')\n",
    "\n",
    "            # get 30 evenly spaced frames per video\n",
    "            step = video.get_meta_data()['nframes'] / frames_per_video\n",
    "            frames_to_get = range(1, video.get_meta_data()['nframes'], int(step))\n",
    "            frames = []\n",
    "            for i in frames_to_get:\n",
    "                frames.append(i)\n",
    "\n",
    "            # only get first 30 frames if there are 31\n",
    "            frames = frames[:30]\n",
    "\n",
    "            # Resize, crop, and save each frame in the Frames folder\n",
    "            ## ex: (os.getcwd + '/videos/Clips/Truthful/Frames') for Truth videos\n",
    "            for frame in frames:\n",
    "                this_frame = video.get_data(frame)\n",
    "                imageio.imwrite(\"this_frame.jpg\", this_frame)\n",
    "                this_frame = Image.open(\"this_frame.jpg\")\n",
    "                this_frame = resize_and_crop(this_frame, frame_square_dim)\n",
    "                this_frame.save((folder + '/Frames/' + video_file + '_0' + str(frame) + '.jpg'))\n",
    "                \n",
    "    print('\\n')\n",
    "    print('Complete!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_data(folder, frames_per_video):\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    for vid_type in os.listdir(folder):\n",
    "        if not vid_type.startswith('.'):\n",
    "            if vid_type in ['Deceptive']:\n",
    "                label = 'Deceptive'\n",
    "            else:\n",
    "                label = 'Truthful'\n",
    "            \n",
    "            video_tensor = []\n",
    "            frame_counter = 1\n",
    "            frame_files = os.listdir(folder + '/' + vid_type + '/Frames')\n",
    "            for frame_file_iter in range(len(frame_files)):\n",
    "                if frame_file_iter == 0:\n",
    "                    continue\n",
    "                \n",
    "                frame = (folder + '/' + vid_type + '/Frames/' + frame_files[frame_file_iter])\n",
    "\n",
    "                frame = Image.open(frame)\n",
    "\n",
    "                frame_tensor = np.asarray(frame.convert('RGB'))\n",
    "                \n",
    "                video_tensor.append(frame_tensor)\n",
    "                \n",
    "                if frame_counter == frames_per_video:\n",
    "                    X.append(video_tensor)\n",
    "                    y.append(label)\n",
    "                    \n",
    "                    video_tensor = []\n",
    "                    frame_counter = 0\n",
    "                    \n",
    "                frame_counter += 1\n",
    "                \n",
    "    X = np.asarray(X)\n",
    "    y = np.asarray(y)\n",
    "    print('Complete!')\n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 62 video files.\n",
      "Getting frames for video 62 of 62\r"
     ]
    }
   ],
   "source": [
    "# Prepare video clips into a resized and cropped series of frames for each video\n",
    "get_frames_from_videos((VIDEO_FOLDER + '/Deceptive'), FRAMES_PER_VIDEO, FRAME_SQUARE_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete!rames for video 61 of 61\n"
     ]
    }
   ],
   "source": [
    "# Prepare video clips into a resized and cropped series of frames for each video\n",
    "get_frames_from_videos((VIDEO_FOLDER + '/Truthful'), FRAMES_PER_VIDEO, FRAME_SQUARE_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete!\n"
     ]
    }
   ],
   "source": [
    "# Get video tensors ready for network input\n",
    "X, y = get_data(VIDEO_FOLDER, FRAMES_PER_VIDEO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Split into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=TEST_SIZE, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Encode target labels\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(y_train)\n",
    "y_train = encoder.transform(y_train)\n",
    "y_test = encoder.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# NEXT IS TO TEST NN\n",
    "# AND THEN TRY IMAGE AUGMENTATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_model(frames_per_video, frame_square_dim, keep_prob):\n",
    "    # Video file placeholder. Shape = (# of frames, frame-width, frame-length, # of frame-channels (i.e. rgb))\n",
    "    video = Input(shape=(frames_per_video, frame_square_dim, frame_square_dim, 3))\n",
    "\n",
    "    # TRANSFER LEARNING LAYER\n",
    "    # Initialize CNN with weights from InceptionV3 trained on Imagenet.\n",
    "    IncV3 = InceptionV3(input_tensor=video, weights='imagenet', include_top=False, pooling='avg')\n",
    "    \n",
    "    # add a global spatial average pooling layer\n",
    "    #x = IncV3.output\n",
    "    #x = GlobalAveragePooling2D()(x)\n",
    "    #x = GlobalAveragePooling2D()(x)\n",
    "\n",
    "    # TIME DISTRIBUTION LAYER\n",
    "    # Run each frame through an InceptionV3 CNN layer.\n",
    "    encoded_frames = TimeDistributed(IncV3)(video)\n",
    "\n",
    "    # LSTM LAYER\n",
    "    # Run each frames CNN output through the LSTM layer.\n",
    "    encoded_vid = LSTM(256)(encoded_frames)\n",
    "\n",
    "    # ADDITIONAL LAYERS TO TRAIN NEW CLASSES from our new video footage.\n",
    "    # Add a fully-connected layer with a dropout before predictions.\n",
    "    x = Dense(1024, activation='relu')(encoded_vid)\n",
    "    x = Dropout(keep_prob)(x)\n",
    "\n",
    "    # Add a logistic layer with 2 new classes - \"Lying\" and \"Truth\".\n",
    "    predictions = Dense(2, activation='sigmoid')(x)\n",
    "\n",
    "    # Throw it all into a Model object.\n",
    "    model = Model(input=IncV3.input, output=predictions)\n",
    "\n",
    "    # Freeze convolutional InceptionV3 layers because we want to start with the it's pre-trained weights.\n",
    "    # This is the \"learning\" that's being \"transferred\".\n",
    "    for layer in IncV3.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    # Compile the model.\n",
    "    model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input 0 is incompatible with layer conv2d_189: expected ndim=4, found ndim=5",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-c4c7f2cce699>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFRAMES_PER_VIDEO\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFRAME_SQUARE_DIM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mKEEP_PROB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-1d6dd9506192>\u001b[0m in \u001b[0;36mget_model\u001b[0;34m(frames_per_video, frame_square_dim, keep_prob)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# TRANSFER LEARNING LAYER\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m# Initialize CNN with weights from InceptionV3 trained on Imagenet.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mIncV3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInceptionV3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_tensor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvideo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'imagenet'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minclude_top\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpooling\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'avg'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m# add a global spatial average pooling layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/keras/applications/inception_v3.py\u001b[0m in \u001b[0;36mInceptionV3\u001b[0;34m(include_top, weights, input_tensor, input_shape, pooling, classes)\u001b[0m\n\u001b[1;32m    165\u001b[0m         \u001b[0mchannel_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconv2d_bn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'valid'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconv2d_bn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'valid'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconv2d_bn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/keras/applications/inception_v3.py\u001b[0m in \u001b[0;36mconv2d_bn\u001b[0;34m(x, filters, num_row, num_col, padding, strides, name)\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0muse_bias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m         name=conv_name)(x)\n\u001b[0m\u001b[1;32m     80\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBatchNormalization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbn_axis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbn_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mActivation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/keras/engine/topology.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    509\u001b[0m                 \u001b[0;31m# Raise exceptions in case the input is not compatible\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m                 \u001b[0;31m# with the input_spec specified in the layer constructor.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 511\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massert_input_compatibility\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m                 \u001b[0;31m# Collect input shapes to build layer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/keras/engine/topology.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    411\u001b[0m                                      \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m': expected ndim='\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    412\u001b[0m                                      \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m', found ndim='\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 413\u001b[0;31m                                      str(K.ndim(x)))\n\u001b[0m\u001b[1;32m    414\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_ndim\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m                 \u001b[0mndim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input 0 is incompatible with layer conv2d_189: expected ndim=4, found ndim=5"
     ]
    }
   ],
   "source": [
    "model = get_model(FRAMES_PER_VIDEO, FRAME_SQUARE_DIM, KEEP_PROB)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# GET BATCHES OR DEFINE GENERATOR\n",
    "\n",
    "# Train the model on the new video footage.\n",
    "model.fit_generator(...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
